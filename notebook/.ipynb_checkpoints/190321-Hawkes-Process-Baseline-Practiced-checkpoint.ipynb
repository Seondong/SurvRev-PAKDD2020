{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook is for implementing Hawkes baseline before scripting\n",
    "* Extended version of Poisson Process with self-simulation and time-decaying\n",
    "* Using [```tick```](https://x-datainitiative.github.io/tick/modules/hawkes.html) library and [```hawkes```](https://github.com/stmorse/hawkes) repo with detail [blog](https://stmorse.github.io/journal/Hawkes-python.html) post \n",
    "* Other reference: https://arxiv.org/pdf/1708.06401.pdf\n",
    "* ```tick``` Citation: @ARTICLE{2017arXiv170703003B,\n",
    "  author = {{Bacry}, E. and {Bompaire}, M. and {Ga{\\\"i}ffas}, S. and {Poulsen}, S.},\n",
    "  title = \"{tick: a Python library for statistical learning, with\n",
    "    a particular emphasis on time-dependent modeling}\",\n",
    "  journal = {ArXiv e-prints},\n",
    "  eprint = {1707.03003},\n",
    "  year = 2017,\n",
    "  month = jul\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('../data/indoor/store_C/train_labels.tsv', sep='\\t')\n",
    "df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfri = df.revisit_interval\n",
    "dfri[dfri > 0].hist(bins=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tick\n",
    "import matplotlib\n",
    "matplotlib.use('Agg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tick.plot import plot_hawkes_kernels\n",
    "from tick.hawkes import SimuHawkesExpKernels, SimuHawkesMulti, HawkesExpKern\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "end_time = 1000\n",
    "n_realizations = 10\n",
    "\n",
    "decays = [[4., 1.], [2., 2.]]\n",
    "baseline = [0.12, 0.07]\n",
    "adjacency = [[.3, 0.], [.6, .21]]\n",
    "\n",
    "hawkes_exp_kernels = SimuHawkesExpKernels(\n",
    "    adjacency=adjacency, decays=decays, baseline=baseline,\n",
    "    end_time=end_time, verbose=False, seed=1039)\n",
    "\n",
    "multi = SimuHawkesMulti(hawkes_exp_kernels, n_simulations=n_realizations)\n",
    "\n",
    "multi.end_time = [(i + 1) / 10 * end_time for i in range(n_realizations)]\n",
    "multi.simulate()\n",
    "\n",
    "learner = HawkesExpKern(decays, penalty='l1', C=10)\n",
    "learner.fit(multi.timestamps)\n",
    "\n",
    "plot_hawkes_kernels(learner, hawkes=hawkes_exp_kernels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "1 dimensional Hawkes process simulation\n",
    "=======================================\n",
    "\"\"\"\n",
    "\n",
    "from tick.plot import plot_point_process\n",
    "from tick.hawkes import SimuHawkes, HawkesKernelSumExp\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "run_time = 40\n",
    "\n",
    "hawkes = SimuHawkes(n_nodes=1, end_time=run_time, verbose=False, seed=1398)\n",
    "kernel = HawkesKernelSumExp([.1, .2, .01], [1., 3., 7.])\n",
    "hawkes.set_kernel(0, 0, kernel)\n",
    "hawkes.set_baseline(0, 1.)\n",
    "\n",
    "dt = 0.1\n",
    "hawkes.track_intensity(dt)\n",
    "hawkes.simulate()\n",
    "timestamps = hawkes.timestamps\n",
    "intensity = hawkes.tracked_intensity\n",
    "intensity_times = hawkes.intensity_tracked_times\n",
    "\n",
    "print(intensity)\n",
    "print(intensity[0].shape)\n",
    "print(intensity_times)\n",
    "\n",
    "\n",
    "\n",
    "_, ax = plt.subplots(1, 3, figsize=(16, 4))\n",
    "plot_point_process(hawkes, n_points=50000, t_min=2, max_jumps=10, ax=ax[0])\n",
    "plot_point_process(hawkes, n_points=50000, t_min=2, t_max=20, ax=ax[1])\n",
    "plot_point_process(hawkes, n_points=50000, t_min=2, t_max=30, ax=ax[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import scipy\n",
    "import scipy.stats\n",
    "size = 30000\n",
    "y = dfri[dfri > 0]\n",
    "plt.figure(figsize=(12,12))\n",
    "plt.hist(y, bins=int(max(y)+1))[-1];\n",
    "dist_names = ['weibull_min']  # , ['lognorm', 'gamma', 'beta', 'weibull_min', 'pareto']\n",
    "for dist_name in dist_names:\n",
    "    dist = getattr(scipy.stats, dist_name)\n",
    "    param = dist.fit(y, floc=0)\n",
    "    pdf_fitted = dist.pdf(x, *param[:-2], loc=param[-2], scale=param[-1]) * size\n",
    "    print(param)\n",
    "    plt.plot(pdf_fitted, label=dist_name)\n",
    "    plt.xlim(0,max(y))\n",
    "plt.legend(loc='upper right')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Trying Hawkes library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from MHP import MHP\n",
    "P = MHP()\n",
    "P.generate_seq(60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "P.plot_events()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Three \n",
    "m = np.array([0.2, 0.0, 0.0])\n",
    "a = np.array([[0.1, 0.0, 0.0], \n",
    "              [0.9, 0.0, 0.0],\n",
    "              [0.0, 0.9, 0.0]])\n",
    "w = 3.1\n",
    "\n",
    "P = MHP(mu=m, alpha=a, omega=w)\n",
    "P.generate_seq(60)\n",
    "P.plot_events()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "P.data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mhat = np.random.uniform(0,1, size=3)\n",
    "ahat = np.random.uniform(0,1, size=(3,3))\n",
    "w = 3.\n",
    "\n",
    "P.EM(ahat, mhat, w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### A single case \n",
    "m = np.array([0.2])\n",
    "a = np.array([[0.1]])\n",
    "w = 5\n",
    "# assert np.round((P.get_rate(P.data[0,0]+1e-10, 0) - m)[0], 4) == w*a\n",
    "\n",
    "P = MHP(mu=m, alpha=a, omega=w)\n",
    "P.generate_seq(3)\n",
    "P.plot_events()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "P.data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mhat = np.random.uniform(0,1, size=1)\n",
    "ahat = np.random.uniform(0,1, size=(1,1))\n",
    "w = 5\n",
    "\n",
    "P.EM(ahat, mhat, w)   # Ahat, mhat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "P.data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### A single case \n",
    "m = np.array([0.1])\n",
    "a = np.array([[0]])\n",
    "w = 1\n",
    "\n",
    "P = MHP(mu=m, alpha=a, omega=w)\n",
    "### In this example, an event appeared in a very early stage of the time horizon.\n",
    "P.generate_seq(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Since the input of the EM method is only P.data, \n",
    "# it does not have any information that no event happens from [1.69, 10]\n",
    "# So the predicted 'mu' value is very big (0.59124238 = 1/1.69135373)\n",
    "P.EM(a, m, w, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Revisit Prediction using Hawkes process\n",
    "\n",
    "Script from here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from MHP import MHP\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "def hawkes_process_baseline(self, data, set):\n",
    "    \"\"\"Hawkes process baseline\n",
    "\n",
    "    Extended version of Poisson Process with self-simulation and time-decaying\n",
    "    Used [Hawkes](https://github.com/stmorse/hawkes) libary\"\"\"\n",
    "    \n",
    "    d = {}\n",
    "    for i,j in enumerate(sorted(list(set(data.df_train.wifi_id.astype(int))))):\n",
    "        d[j] = i\n",
    "    \n",
    "    df_train.wifi_id = df_train.wifi_id.astype(int).apply(lambda x: d[x])\n",
    "    train_start_time = min(data.df_train.ts_start)\n",
    "    train_end_time = max(data.df_train.ts_end)\n",
    "    test_start_time = min(data.df_test.ts_start)\n",
    "    test_end_time = min(data.df_test.ts_end)\n",
    "    num_days = (train_end_time-train_start_time) / 86400\n",
    "\n",
    "    \n",
    "    \"\"\"Train phase\"\"\"\n",
    "    # For referring train data in the prediction, we first collect train customer's visit rate and self-stimulation rate.\n",
    "    data = np.array([(data.df_train.ts_start-train_start_time)/86400, data.df_train.wifi_id]).transpose()\n",
    "    # Split by wifi_id (sorted) -> To put each trajectory into EM model separately \n",
    "    data_splitted = np.split(data, np.where(np.diff(data[:, 1]))[0]+1)\n",
    "\n",
    "    all_params = []\n",
    "    for edata in data_splitted:\n",
    "        edata[:,1] = 0\n",
    "\n",
    "        m = np.array([len(edata) / num_days])\n",
    "        a = np.array([[0.1]])\n",
    "        w = 0.5\n",
    "\n",
    "        P = MHP(mu=m, alpha=a, omega=w) \n",
    "        P.data = edata\n",
    "        \n",
    "        # EM algorithm to estimate parameters\n",
    "        ahat,mhat = P.EM(a, m, w, verbose=False)  \n",
    "\n",
    "        # Save new parameters to MHP instance\n",
    "        P.mu = m\n",
    "        P.alpha = ahat\n",
    "    #     print('{}, {:2f}, {}, {}'.format(len(edata), num_days, ahat, m))\n",
    "        all_params.append((ahat, m, P)) # Keep ahat, mhat from training set  (shuld be mhat instead m if Hawkes EM function works with an additional input of time horizon t)\n",
    "    \n",
    "    \n",
    "    \"\"\"Test case\"\"\"\n",
    "    # Predicting revisit of test customers by referring visit rate and self-stimulation rate from train customers.\n",
    "    y_pred_regr_all = []\n",
    "    for t in list(data.df_test.ts_end):\n",
    "        remaining_time = (test_end_time-t)/86400\n",
    "        ahat, _, _ = random.choice(all_params)\n",
    "        mhat = np.array([86400/(t-train_start_time)])\n",
    "    #     print('Remaining time: {}, Ahat: {}, Mhat: {}'.format(remaining_time, ahat, mhat))\n",
    "        P = MHP(mu=mhat, alpha=ahat, omega=w) \n",
    "        P.generate_seq(remaining_time*10)\n",
    "    #     print(P.data)\n",
    "        try:\n",
    "            rint = P.data[0][0]\n",
    "            rbin = int(remaining_time >= rint)\n",
    "        except IndexError:\n",
    "            rint = np.inf\n",
    "            rbin = 0\n",
    "        y_pred_regr_all.append(rint)\n",
    "        \n",
    "        \n",
    "    \"\"\"Train censored case\"\"\"\n",
    "    y_pred_regr_all = []\n",
    "    for t, params in zip(list(data.df_train_censored.ts_end), hats):\n",
    "        remaining_time = (test_end_time-test_start_time)/86400\n",
    "        no_event_time = (test_start_time - t)/86400\n",
    "\n",
    "        P = all_params[-1]\n",
    "        rel_time = (test_start_time-train_start_time)/86400\n",
    "    #     print(t, params, P.data, rel_time)\n",
    "    #     print('Rate: ', P.get_rate(rel_time, 0))\n",
    "    #     print('Difference between original rate: {}'.format(P.get_rate(rel_time, 0) - params[1][0]))\n",
    "\n",
    "        P.mu = params[1]\n",
    "        P.alpha = params[0]\n",
    "        predicted = P.generate_seq(remaining_time)\n",
    "\n",
    "        try:\n",
    "            rint = predicted[0][0]\n",
    "            rbin = int(remaining_time >= rint)\n",
    "        except IndexError:\n",
    "            rint = np.inf\n",
    "            rbin = 0\n",
    "        y_pred_regr_all.append(rint)\n",
    "        \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "        \n",
    "        \n",
    "\n",
    "    # Case for test instances in testing timeframe\n",
    "    if set =='test':\n",
    "        # left-censored time for testing set, which can be equivalent to 1/lambda(=mu) for each user\n",
    "        mu = (data.df_test.ts_start - data_start)\n",
    "        mu /= data.df_test.nvisits\n",
    "        y_pred_regr_all = mu.apply(np.random.exponential)\n",
    "        y_pred_clas = data.df_test.ts_end + y_pred_regr_all > data.last_timestamp\n",
    "        y_test_clas = np.asarray(data.df_test['revisit_intention'])\n",
    "\n",
    "        y_pred_regr_all = mu.apply(np.random.exponential) / 86400\n",
    "        test_uncensored_indices = data.df_test.revisit_interval.notnull()\n",
    "        y_pred_regr = y_pred_regr_all[test_uncensored_indices]\n",
    "        y_test_regr = np.array(data.df_test['revisit_interval'][test_uncensored_indices])\n",
    "\n",
    "        acc = sklearn.metrics.accuracy_score(y_test_clas, y_pred_clas)\n",
    "        rmse = utils.root_mean_squared_error(y_test_regr, y_pred_regr)\n",
    "        cindex = lifelines.utils.concordance_index(event_times=data.test_suppress_time,\n",
    "                               predicted_scores=y_pred_regr_all,\n",
    "                               event_observed=(data.test_labels['revisit_intention'] == 1))\n",
    "        fscore = sklearn.metrics.f1_score(y_test_clas, y_pred_clas)\n",
    "\n",
    "    # Case for train_censored instances for their prediction\n",
    "    elif set == 'train_censored':\n",
    "        # For train_censored set, we have an observation until t1 (last time of the train data),\n",
    "        # so we can make an equation as in the below line.\n",
    "        mu = max(data.df_train.ts_end) - data_start\n",
    "        mu /= data.df_train_censored.nvisits\n",
    "\n",
    "        y_pred_regr_all = mu.apply(np.random.exponential)\n",
    "\n",
    "        y_pred_clas = data.df_train_censored.ts_end + y_pred_regr_all > data.last_timestamp\n",
    "        y_test_clas = np.asarray(data.df_train_censored['revisit_intention'])\n",
    "\n",
    "        y_pred_regr_all = mu.apply(np.random.exponential) / 86400\n",
    "\n",
    "        test_uncensored_indices = data.df_train_censored.revisit_interval.notnull()\n",
    "        y_pred_regr = y_pred_regr_all[test_uncensored_indices]\n",
    "        y_test_regr = np.array(data.df_train_censored['revisit_interval'][test_uncensored_indices])\n",
    "\n",
    "        acc = sklearn.metrics.accuracy_score(y_test_clas, y_pred_clas)\n",
    "        rmse = utils.root_mean_squared_error(y_test_regr, y_pred_regr)\n",
    "        cindex = lifelines.utils.concordance_index(event_times=data.train_censored_new_suppress_time,\n",
    "                                                   predicted_scores=y_pred_regr_all,\n",
    "                                                   event_observed=(data.train_censored_actual_labels['revisit_intention'] == 1))\n",
    "        fscore = sklearn.metrics.f1_score(y_test_clas, y_pred_clas)\n",
    "\n",
    "    result = {'acc': acc, 'rmse': rmse, 'cindex': cindex, 'fscore': fscore}\n",
    "    return result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "df_train = pd.read_pickle('../survival-revisit-code/df_train.p')\n",
    "\n",
    "d = {}\n",
    "for i,j in enumerate(sorted(list(set(df_train.wifi_id.astype(int))))):\n",
    "    d[j] = i\n",
    "    \n",
    "df_train.wifi_id = df_train.wifi_id.astype(int).apply(lambda x: d[x])\n",
    "train_end_time = max(df_train.ts_end)\n",
    "train_start_time = min(df_train.ts_start)\n",
    "num_days = (train_end_time-train_start_time) / 86400\n",
    "\n",
    "# For referring train data in the prediction, we first\n",
    "# collect train customer's visit rate and self-stimulation rate.\n",
    "\n",
    "data = np.array([(df_train.ts_start-min(df_train.ts_start))/86400, df_train.wifi_id]).transpose()\n",
    "# Split by wifi_id (sorted) -> To put each trajectory into EM model separately \n",
    "data_splitted = np.split(data, np.where(np.diff(data[:, 1]))[0]+1)\n",
    "\n",
    "hats = []\n",
    "for edata in data_splitted:\n",
    "    edata[:,1] = 0\n",
    "    \n",
    "    m = np.array([len(edata) / num_days])\n",
    "    a = np.array([[0.1]])\n",
    "    w = 0.5\n",
    "    \n",
    "    P = MHP(mu=m, alpha=a, omega=w) \n",
    "    P.data = edata\n",
    "    \n",
    "    ahat,mhat = P.EM(a, m, w, verbose=False)  \n",
    "    \n",
    "    # Save new one\n",
    "    P.mu = m\n",
    "    P.alpha = ahat\n",
    "#     print('{}, {:2f}, {}, {}'.format(len(edata), num_days, ahat, m))\n",
    "    hats.append((ahat, m, P)) # Keep ahat, mhat from training set  (shuld be mhat instead m if Hawkes EM function works with an additional input of time horizon t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Predicting revisit of test customers by referring visit rate and self-stimulation rate from train customers.\n",
    "import random\n",
    "df_test = pd.read_pickle('../survival-revisit-code/df_test.p')\n",
    "test_end_time = max(df_test.ts_end)\n",
    "train_start_time = min(df_train.ts_start)\n",
    "for t in list(df_test.ts_end):\n",
    "    remaining_time = (test_end_time-t)/86400\n",
    "    ahat, _, _ = random.choice(hats)\n",
    "    mhat = np.array([86400/(t-train_start_time)])\n",
    "#     print('Remaining time: {}, Ahat: {}, Mhat: {}'.format(remaining_time, ahat, mhat))\n",
    "    P = MHP(mu=mhat, alpha=ahat, omega=w) \n",
    "    P.generate_seq(remaining_time)\n",
    "#     print(P.data)\n",
    "    try:\n",
    "        rint = P.data[0][0]\n",
    "        rbin = int(remaining_time >= rint)\n",
    "    except IndexError:\n",
    "        rint = np.inf\n",
    "        rbin = 0\n",
    "#     print(rint, rbin)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(data_splitted), len(df_train_censored), len(hats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "P.get_rate(100, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(hats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train_censored.ts_end.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predicting revisit of train_censored customers by referring visit rate and self-stimulation rate from train customers.\n",
    "import random\n",
    "df_train_censored = df_train.drop_duplicates(subset=['wifi_id'], keep='last')\n",
    "test_end_time = max(df_test.ts_end)\n",
    "test_start_time = min(df_test.ts_start)\n",
    "train_start_time = min(df_train.ts_start)\n",
    "\n",
    "for t, params in zip(list(df_train_censored.ts_end), hats):\n",
    "    remaining_time = (test_end_time-test_start_time)/86400\n",
    "    no_event_time = (test_start_time - t)/86400\n",
    "    \n",
    "    P = params[-1]\n",
    "    rel_time = (test_start_time-train_start_time)/86400\n",
    "#     print(t, params, P.data, rel_time)\n",
    "#     print('Rate: ', P.get_rate(rel_time, 0))\n",
    "#     print('Difference between original rate: {}'.format(P.get_rate(rel_time, 0) - params[1][0]))\n",
    "    \n",
    "    P.mu = params[1]\n",
    "    P.alpha = params[0]\n",
    "    predicted = P.generate_seq(remaining_time)\n",
    "\n",
    "    try:\n",
    "        rint = predicted[0][0]\n",
    "        rbin = int(remaining_time >= rint)\n",
    "    except IndexError:\n",
    "        rint = np.inf\n",
    "        rbin = 0\n",
    "    print(rint, rbin)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
